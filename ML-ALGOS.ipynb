{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CLASS IMBALANCE HANDLING AND STRATIFIED SPLIT ----\n",
    "\n",
    "def rebalance(X, y):\n",
    "  \"\"\" Function rebalances given dataset by oversampling the minority class to match the count of the majority class \n",
    "\n",
    "  Args:\n",
    "      X (array): Feature Matrix\n",
    "      y (array): Target Vector\n",
    "  \n",
    "  Returns:\n",
    "      new_X (array): Rebalenced Feature Matrix\n",
    "      new_y (array): Rebalenced Target Vector\n",
    "  \"\"\"\n",
    "  \n",
    "  # Getting Counts of Each Class and finding the size of the imbalence\n",
    "  classes, counts = np.unique(y, return_counts = True)\n",
    "  class_counts = dict(zip(classes, counts))\n",
    "  minority_class = min(class_counts, key = class_counts.get)\n",
    "  majority_class = max(class_counts, key = class_counts.get)\n",
    "  imbalence = class_counts[majority_class] - class_counts[minority_class]\n",
    "  \n",
    "  # Finding Indices of Minority class\n",
    "  minority_indices = np.where(y == minority_class)[0]\n",
    "  \n",
    "  # Randomly oversampling the required amount of points\n",
    "  oversample = np.random.choice(minority_indices, size = imbalence)\n",
    "  \n",
    "  # Applying the oversampling to the feature matrix and target vector\n",
    "  new_X = np.concatenate([X, X[oversample]], axis = 0)\n",
    "  new_y = np.concatenate([y, y[oversample]], axis = 0)\n",
    "    \n",
    "  return new_X, new_y\n",
    "\n",
    "def rebalanced_stratified_split(X_orig, y_orig, test_size = 0.2, random_state = None):\n",
    "  \"\"\" Function performs a stratified split on the rebalanced dataset. First it rebalences dataset by calling\n",
    "  'rebalance()' function then it performs the stratified split ensuring that class distribution is preserved in \n",
    "  both the train and test sets. \n",
    "  \n",
    "  Args:\n",
    "      X_orig (array): Original Feature Matrix \n",
    "      y_orig (array): Original Target Vector\n",
    "      test_size (float, optional): Size of test set. Defaults to 0.2.\n",
    "      random_state (int, optional): Seed for random number generation for reproducibility. Defaults to None.\n",
    "    \n",
    "  Returns: \n",
    "      X_train (array): Feature matrix for training\n",
    "      X_test (array):Feature matrix for testing\n",
    "      y_train (array): Target vector for training\n",
    "      y_test (array): Target vector for testing\n",
    "      \"\"\"\n",
    "      \n",
    "  # Setting random seed\n",
    "  np.random.seed(random_state)\n",
    "  \n",
    "  # First we rebalence the inputs\n",
    "  X, y = rebalance(X_orig, y_orig)\n",
    "  \n",
    "  # Stratified split means we divide into test train while maintaining equal class distribution in both sets\n",
    "  test_size = round(len(y) * test_size)\n",
    "  train_size = len(y) - test_size\n",
    "  \n",
    "  # Assuming binary problem\n",
    "  classes = np.unique(y, return_counts = False)\n",
    "  neg, pos = classes[0], classes[1]\n",
    "  \n",
    "  # Finding Indices of Each Class\n",
    "  p_indices = np.where(y == pos)[0]\n",
    "  n_indices = np.where(y == neg)[0]\n",
    "  \n",
    "  # Randomly populating test set then finding the leftover train set each with equal class balence\n",
    "  p_test_indices = np.random.choice(p_indices, size = int(np.ceil(test_size/2)))\n",
    "  n_test_indices = np.random.choice(n_indices, size = int(np.ceil(test_size/2)))\n",
    "  p_train_indices = np.setdiff1d(p_indices, p_test_indices)\n",
    "  n_train_indices = np.setdiff1d(n_indices, n_test_indices)\n",
    "  \n",
    "  # Retreiving the test set\n",
    "  X_test = np.concatenate([X[p_test_indices], X[n_test_indices]], axis = 0)\n",
    "  y_test = np.concatenate([y[p_test_indices], y[n_test_indices]], axis = 0) \n",
    "  \n",
    "  # Retreiving the train set\n",
    "  X_train = np.concatenate([X[p_train_indices], X[n_train_indices]], axis = 0)\n",
    "  y_train = np.concatenate([y[p_train_indices], y[n_train_indices]], axis = 0)\n",
    "  \n",
    "  # Re-shuffling the train and test feature matrix and target vector\n",
    "  test_permutation = np.random.permutation(test_size)\n",
    "  train_permutation = np.random.permutation(train_size)\n",
    "  X_test = X_test[test_permutation]\n",
    "  y_test = y_test[test_permutation]\n",
    "  X_train = X_train[train_permutation]\n",
    "  y_train = y_train[train_permutation]\n",
    "  \n",
    "  return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BASIC CLASSIFIER PERFORMANCE METRICS ----\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "  \"\"\" Calculates the accuracy of a classifier's predictions e.g. the ratio of correctly predicted instances to the\n",
    "  total number of instances\n",
    "\n",
    "  Args:\n",
    "      y_true (array): Array of true labels\n",
    "      y_pred (array): Array of predicted labels\n",
    "    \n",
    "  Returns:\n",
    "      accuracy (float): Accuracy score\n",
    "  \"\"\"\n",
    "  \n",
    "  # Number of correct predictions\n",
    "  correct_preds = np.sum(y_pred == y_true)\n",
    "  \n",
    "  # Total number of predictions\n",
    "  total_preds = len(y_pred)\n",
    "  \n",
    "  # Ratio of correct preds to total preds i.e. accuracy score\n",
    "  accuracy = correct_preds / total_preds\n",
    "  \n",
    "  return accuracy\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    \"\"\" Function computes the confusion matrix based on the true and predicted labels provided. Rows represent\n",
    "    predicted labels and columns represent true labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (array): Array of true labels\n",
    "        y_pred (array): Array of predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        conf_matrix (array): Confusion Matrix\n",
    "    \"\"\"\n",
    "    # Finding number of classes\n",
    "    num_classes = len(set(y_true))\n",
    "    \n",
    "    # Initialising confusion matrix populated with zeros\n",
    "    conf_matrix = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    # Populate confusion matrix with true on rows and predictions on columns similarly to coordinates\n",
    "    for x, y in zip(y_true, y_pred):\n",
    "        conf_matrix[x][y] += 1\n",
    "        \n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- LINEAR CLASSIFIER (LOGISTIC REGRESSION) OBJECT ----\n",
    "\n",
    "class LinearClassifier:\n",
    "    \"\"\" Class can fit a linear decision boundary to a given dataset and make predictions based on this boundary\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold = 0.5):\n",
    "        \"\"\" Constructor initialises the classifier object\n",
    "        \n",
    "        Args: \n",
    "            threshold (float): Detirmines the decision threshold. Default is 0.5\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.weights = None\n",
    "        self.lr = None\n",
    "        self.epochs = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Method trains the linear classifier using the training data by estimating the weights (coefficients)\n",
    "        and bias term of the decision boundary\n",
    "\n",
    "        Args:\n",
    "            X (array): Data Matrix\n",
    "            y (array): Target Vector\n",
    "        \"\"\"\n",
    "        # Initialising bias, weights, and sigmoid function\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X, ones), axis = 1)\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        # Gradient Descent\n",
    "        self.epochs = 1000\n",
    "        self.lr = 0.01\n",
    "        \n",
    "        # For epochs\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            # Calculating predictions\n",
    "            lm = np.dot(X, self.weights)\n",
    "            preds = sigmoid(lm)\n",
    "            \n",
    "            # Computing error\n",
    "            error = preds - y \n",
    "            \n",
    "            # Computing gradients of loss function (w.r.t weights) by definition\n",
    "            dw = (1 / X.shape[0]) * np.dot(X.T, error)\n",
    "            \n",
    "            # Updating weights\n",
    "            self.weights -=  self.lr * dw\n",
    "           \n",
    "    def predict(self, X):\n",
    "        \"\"\" Method predicts labels for the given inputs based on trained classifier (LM's outputs) considering the\n",
    "        decision threshold specified during initialisation\n",
    "\n",
    "        Args:\n",
    "            X (array): Data Matrix\n",
    "        \"\"\"\n",
    "        # Initialising bias, threshold and sigmoid function\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X, ones), axis = 1)\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        threshold = self.threshold\n",
    "\n",
    "        # Calculating predictions\n",
    "        lm = np.dot(X, self.weights)\n",
    "        probs = sigmoid(lm)\n",
    "        \n",
    "        # Getting predictions considering the threshold\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- K-NEAREST NEIGHBOURS (KNN) CLASSIFIER AND KNN VARIANTS ----\n",
    "\n",
    "class KNNClassifier:\n",
    "  \"\"\" Here we implement a K-Nearest Neighbours Classifier which is capable of making predictions based on the majority\n",
    "  class of the K nearest neighbours\"\"\"\n",
    "  \n",
    "  def __init__(self, k=3):\n",
    "    \"\"\" Constructor method to initialise the classifier object. \n",
    "    \n",
    "    Args: \n",
    "      k (int): Represents the number of neighbours to consider. Default is 3\n",
    "    \"\"\"\n",
    "    self.k = k\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    \"\"\" Fits the parameters of the model. \n",
    "\n",
    "    Args:\n",
    "        X (array): Data Matrix\n",
    "        y (array): Target Vector\n",
    "    \"\"\"\n",
    "    self.X_train = X\n",
    "    self.y_train = y\n",
    "    \n",
    "  def euclidean_distance(self, x1, x2):\n",
    "    \"\"\" Calculates the Euclidean distance between two data points\n",
    "\n",
    "    Args:\n",
    "        x1 (array): Arbitrary Vector \n",
    "        x2 (array): Arbitrary Vector\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.square(x1 - x2)))\n",
    "    \n",
    "  def predict_prob_single(self, x):\n",
    "    \"\"\" Predicts the probability of each class label for a single data point based on its nearest neighbours in the dataset\n",
    "\n",
    "    Args:\n",
    "        x (array): Arbitrary Vector\n",
    "    \"\"\"\n",
    "    # Finding distance between point and all other points using euclidean distance\n",
    "    distances = np.zeros(len(self.X_train))\n",
    "    for i, point in enumerate(self.X_train):\n",
    "      distances[i] = self.euclidean_distance(x, point)\n",
    "    \n",
    "    # Finding k-nearest neighbours\n",
    "    neighbours = np.argsort(distances)\n",
    "    knn = neighbours[:self.k]\n",
    "    knn_labels = self.y_train[knn]\n",
    "  \n",
    "    # Calculating probability of each class label\n",
    "    unique_labels = np.unique(self.y_train, return_counts = False)\n",
    "    counts = np.zeros(len(unique_labels))\n",
    "  \n",
    "    for label in knn_labels:\n",
    "      counts[label] += 1\n",
    "      \n",
    "    probabilities = counts / self.k\n",
    "\n",
    "    return probabilities\n",
    "    \n",
    "  def predict(self, X):\n",
    "    \"\"\" Predicts the class labels for a given set of test data points based on the majority class of their nearest neighbours\n",
    "\n",
    "    Args:\n",
    "        X (array): Data Matrix\n",
    "    \"\"\"\n",
    "    predictions = np.zeros(len(X))\n",
    "    \n",
    "    # We can equivalently find the class with the highest percentage \n",
    "    for i, point in enumerate(X):\n",
    "      probs = self.predict_prob_single(point)\n",
    "      max_prob = np.argmax(probs)\n",
    "      predictions[i] = max_prob\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "class CostSensitiveKNNClassifier:\n",
    "  \"\"\" This class implements a cost-sensitive knn classifier class which can make predictions based on the majority class of \n",
    "  the k-nearest neighbours while simultaneously considering class costs\"\"\"\n",
    "\n",
    "  def __init__(self, k = 3, class_cost_matrix = [[0,1], [1,0]]):\n",
    "    \"\"\" Constructor method to initialise classifier object\n",
    "\n",
    "    Args:\n",
    "        k (int): Represents the number of neighbours to consider. Default is 3\n",
    "        class_cost_matrix (array): Cost Matrix. Default is [[0,1],[1,0]]\n",
    "    \"\"\"\n",
    "    self.k = k\n",
    "    self.cm = class_cost_matrix\n",
    "      \n",
    "  def fit(self, X, y):\n",
    "    \"\"\" Method fits the parameters of the model\n",
    "\n",
    "    Args:\n",
    "        X (array): Data Matrix\n",
    "        y (array): Target Vector\n",
    "    \"\"\"\n",
    "    self.X_train = X\n",
    "    self.y_train = y\n",
    "    \n",
    "  def euclidean_distance(self, x1, x2):\n",
    "    \"\"\" Calculates the Euclidean distance between two data points\n",
    "\n",
    "    Args:\n",
    "        x1 (array): Arbitrary Vector \n",
    "        x2 (array): Arbitrary Vector\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.square(x1 - x2)))\n",
    "  \n",
    "  def predict_prob_single(self, x):\n",
    "    \"\"\" Predicts the probability of each class label for a sinlge data point based on its nearest neighbours in the dataset\n",
    "\n",
    "    Args:\n",
    "        x (array): Arbitrary Vector\n",
    "    \"\"\"\n",
    "    # Finding distance between point and all other points using euclidean distance\n",
    "    distances = np.zeros(len(self.X_train))\n",
    "    for i, point in enumerate(self.X_train):\n",
    "      distances[i] = self.euclidean_distance(x, point)\n",
    "    \n",
    "    # Finding k-nearest neighbours\n",
    "    neighbours = np.argsort(distances)\n",
    "    knn = neighbours[:self.k]\n",
    "    knn_labels = self.y_train[knn]\n",
    "  \n",
    "    # Calculating probability of each class label\n",
    "    unique_labels = np.unique(self.y_train, return_counts = False)\n",
    "    counts = np.zeros(len(unique_labels))\n",
    "\n",
    "    for label in knn_labels:\n",
    "      counts[label] += 1\n",
    "    \n",
    "    probabilities = counts / self.k\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\" Predicts the class labels for a given set of points based on class costs and class counts of nearest neighbours\n",
    "\n",
    "    Args:\n",
    "        X (array): Data Matrix\n",
    "    \"\"\"\n",
    "    predictions = np.zeros(len(X))\n",
    "    \n",
    "    for i, point in enumerate(X):\n",
    "      \n",
    "      probs = self.predict_prob_single(point)\n",
    "     \n",
    "      # Costs of prediction: rows = predictions, columns = true i.e. 0,0 is cost of predicting 0 when actual is 0...\n",
    "      ovr = probs @ self.cm\n",
    "      \n",
    "      # Finally finding neighbour with minimum cost\n",
    "      max_ovr = np.argmin(ovr)\n",
    "\n",
    "      predictions[i] = max_ovr\n",
    "      \n",
    "    return predictions\n",
    "\n",
    "class GroupKNNClassifier:\n",
    "    \"\"\" This class implements a Group weighted KNN classifier. Unlike other KNNs which treat all features equally, this class divides\n",
    "    features into groups and assigns different weights to these groups. This allows the classifier to emphasize certain groups of features \n",
    "    over others which is useful when some features are known to be more informative\"\"\"\n",
    "    \n",
    "    def __init__(self, k, groups, group_weights):\n",
    "      \"\"\" Constructor method initialises the classifier object \n",
    "\n",
    "      Args:\n",
    "          k (int): Number of nearest neighbours to consider when making predictions\n",
    "          groups (list): List of Lists where each sublist contains the indices of features belonging to a particular group\n",
    "          group_weights (list): The weights asigned to each group\n",
    "      \"\"\"\n",
    "      self.k = k\n",
    "      self.groups = groups\n",
    "      self.weights = group_weights\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "      \"\"\" Method fits the parameters of the model - Stores training data for use in prediction\n",
    "\n",
    "        Args:\n",
    "          X (array): Data Matrix\n",
    "          y (array): Target Vector\n",
    "      \"\"\"\n",
    "      self.X_train = X\n",
    "      self.y_train = y\n",
    "    \n",
    "    def euclidean_distance(self, x1, x2):\n",
    "      \"\"\" Calculates the Euclidean distance between two data points\n",
    "\n",
    "      Args:\n",
    "          x1 (array): Arbitrary Vector \n",
    "          x2 (array): Arbitrary Vector\n",
    "      \"\"\"\n",
    "      return np.sqrt(np.sum(np.square(x1 - x2)))\n",
    "    \n",
    "    def distance(self, x1, x2):\n",
    "      \"\"\" Method calculates the weighted distance between two data points considering the different groups of features and their \n",
    "      respective weights.\n",
    "\n",
    "      Args:\n",
    "          x1 (array): Arbitrary Vector\n",
    "          x2 (array): Arbitrary Vector\n",
    "      \"\"\"\n",
    "      \n",
    "      weighted_distance = 0\n",
    "      # Iterate through feature-weight pairs\n",
    "      for features, weighting in zip(self.groups, self.weights):\n",
    "        # Iterate  through features in group to handle case where there are multiple features\n",
    "        for feature in features:\n",
    "          # Calculates squared difference for current feature\n",
    "          squared_difference = np.square(x1[feature] - x2[feature])\n",
    "          # Applies corresponding weighting and adds to sum for whole loop\n",
    "          weighted_distance += squared_difference * weighting\n",
    "      \n",
    "      # Square roots the sum to find final weighted difference \n",
    "      return np.sqrt(weighted_distance)\n",
    "\n",
    "    def predict(self, X):\n",
    "      \"\"\" Method predicts the class labels for a given set of test data points considering the group-weighted distances. \n",
    "\n",
    "      Args:\n",
    "          X (array): Data Matrix\n",
    "      \"\"\"\n",
    "      predictions = np.zeros(len(X))\n",
    "      \n",
    "      # Finding weighted distance between each point and all training points\n",
    "      for i, point in enumerate(X):\n",
    "        \n",
    "        # Initialise array to store distances from current test point to all train points\n",
    "        point_distances = np.zeros(len(self.X_train))\n",
    "        for j, train_point in enumerate(self.X_train):\n",
    "          # Calculates weighted distance\n",
    "          point_distances[j] = self.distance(point, train_point)\n",
    "\n",
    "        # Finds k-nearest neighbours w.r.t weighted distance\n",
    "        neighbours = np.argsort(point_distances)\n",
    "        knn = neighbours[:self.k]\n",
    "        knn_labels = self.y_train[knn]\n",
    "        \n",
    "        # Calculating probability of each class label based on counts\n",
    "        unique_labels = np.unique(self.y_train, return_counts = False)\n",
    "        counts = np.zeros(len(unique_labels))\n",
    "        \n",
    "        for label in knn_labels:\n",
    "          counts[label] += 1\n",
    "        \n",
    "        probabilities = counts / self.k\n",
    "        \n",
    "        # Populates prediction for current point with highest probability class label based on weighted nearest neighbours \n",
    "        predictions[i] = np.argmax(probabilities)\n",
    "        \n",
    "      return predictions\n",
    "\n",
    "class AutoGroupsKNNClassifier:\n",
    "    \"\"\" This class implements Auto-Group KNN classifier which automatically divides features into 2 groups (one for important features and one for less-important features) \n",
    "    and assigns weights to these groups to emphasize their importance. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k = 3, param = 0.8, weight = 0.2):\n",
    "      \"\"\" Constructor method initialises the classifier object\n",
    "\n",
    "      Args:\n",
    "          k (int): Represents the number of neighbours to consider. Defaults to 3.\n",
    "          param (float): Defines the threshold which defines the two groups. Defaults to 0.8.\n",
    "          weight (float): Defines the weight that is given to less important features, and 1-weight is weight given to important features. Defaults to 0.2.\n",
    "      \"\"\"\n",
    "      self.k = k \n",
    "      self.threshold = param\n",
    "      self.groups = None\n",
    "      self.weights = [(1 - weight), weight]\n",
    "      \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "      \"\"\" Trains the classifier using given data and automatically selects feature groups based on the provided threshold\n",
    "\n",
    "      Args:\n",
    "          X (array): Data matrix\n",
    "          y (array): Target Vector\n",
    "      \"\"\"\n",
    "      # We have positive and negative data stacked on top of eachother within the informative and non-informative features so mean is useless\n",
    "      # We chose to use standard deviation for our notion of importance\n",
    "      feature_stds = np.std(X, axis = 0)\n",
    "\n",
    "      # Finding index of features above and below threshold\n",
    "      important_feature_indices = np.where(feature_stds > self.threshold)[0]\n",
    "      unimportant_feature_indices = np.where(feature_stds <= self.threshold)[0]\n",
    "      self.groups = [list(important_feature_indices), list(unimportant_feature_indices)]\n",
    "      \n",
    "      self.X_train = X\n",
    "      self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "      \"\"\" Predicts the class labels for given set of test data points using the trained classifier\n",
    "\n",
    "      Args:\n",
    "          X (array): Data matrix\n",
    "      \"\"\"\n",
    "      predictions = np.zeros(len(X))\n",
    "      \n",
    "      # Finding weighted distance between each point and all training points\n",
    "      for i, point in enumerate(X):\n",
    "        \n",
    "        # Initialise array to store distances from current test point to all train points\n",
    "        point_distances = np.zeros(len(self.X_train))\n",
    "        for j, train_point in enumerate(self.X_train):\n",
    "          \n",
    "          # Calculate weighted distance\n",
    "          weighted_distance = 0\n",
    "          # Iterate through feature-weight pairs\n",
    "          for features, weighting in zip(self.groups, self.weights):\n",
    "            # Iterate  through features in group to handle case where there are multiple features\n",
    "            for feature in features:\n",
    "              # Calculates squared difference for current feature\n",
    "              squared_difference = np.square(point[feature] - train_point[feature])\n",
    "              # Applies corresponding weighting and adds to sum for whole loop\n",
    "              weighted_distance += squared_difference * weighting\n",
    "\n",
    "          # Square roots the sum to find final weighted difference \n",
    "          point_distances[j] = np.sqrt(weighted_distance)\n",
    "\n",
    "        # Finds k-nearest neighbours w.r.t weighted distance\n",
    "        neighbours = np.argsort(point_distances)\n",
    "        knn = neighbours[:self.k]\n",
    "        knn_labels = self.y_train[knn]\n",
    "        \n",
    "        # Calculating probability of each class label based on counts\n",
    "        unique_labels = np.unique(self.y_train, return_counts = False)\n",
    "        counts = np.zeros(len(unique_labels))\n",
    "        \n",
    "        for label in knn_labels:\n",
    "          counts[label] += 1\n",
    "        \n",
    "        probabilities = counts / self.k\n",
    "        \n",
    "        # Populates prediction for current point with highest probability class label based on weighted nearest neighbours \n",
    "        predictions[i] = np.argmax(probabilities)\n",
    "        \n",
    "      return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- ADAPTIVE BOOSTING CLASSIFIER (ADABOOST) ----\n",
    "\n",
    "def train_ab(X_train, y_train, param):\n",
    "    \"\"\" Function trains an AdaBoost ensemble using decision tree classifiers as base learners. It iteratively updates the weights \n",
    "    of the training instances  based on their performance and combines multiple weak learners into a strong learner.\n",
    "\n",
    "    Args:\n",
    "        X_train (array): Training feature matrix\n",
    "        y_train (array): Training labels\n",
    "        param (int): Maximum depth of decision trees\n",
    "    \n",
    "    Returns: \n",
    "        model (list): List of tuples containing the trained/learned models and their corresponding weights  \n",
    "    \"\"\"\n",
    "    n_instances = len(X_train)\n",
    "    # Initialising weights arrays for training instances (uniform summing to one meaning equal probability) and learner, weight tuples to be appended too\n",
    "    training_weights = np.ones(n_instances) / n_instances\n",
    "    models_weights = []\n",
    "    \n",
    "    # We use 100 weak learners but this is down to preference\n",
    "    for _ in range(100):\n",
    "        \n",
    "        # Training weak learner on weighted samples \n",
    "        weak_learner = DecisionTreeClassifier(max_depth = param)\n",
    "        weak_learner.fit(X_train, y_train, training_weights)\n",
    "\n",
    "        # Makes predictions\n",
    "        preds = weak_learner.predict(X_train)\n",
    "        \n",
    "        # Calculating error\n",
    "        accuracy = np.sum(training_weights[y_train == preds]) / np.sum(training_weights)\n",
    "        error = 1.0 - accuracy\n",
    "\n",
    "        # Calculates learner weighting by definition - called alpha in literature - added division by 0 handling  \n",
    "        learner_weight = 0.5 * np.log((1 - error) / max(error, 1e-10))\n",
    "        \n",
    "        # Updates training weights by definition and re-normalises them\n",
    "        training_weights = training_weights * np.exp(learner_weight * (preds != y_train))\n",
    "        training_weights = training_weights / np.sum(training_weights)\n",
    "        \n",
    "        # Stores learner and weight\n",
    "        models_weights.append((weak_learner, learner_weight))\n",
    "    \n",
    "    return models_weights\n",
    "\n",
    "def test_ab(X_test, models):\n",
    "    \"\"\" Function makes predictions on the test data using the trained AdaBoost ensemble model.\n",
    "\n",
    "    Args:\n",
    "        X_test (array): Test feature matrix\n",
    "        models (list): List of tuples containing the learned models and their corresponding weights\n",
    "    \n",
    "    Returns: \n",
    "        predictions (array) Predicted labels for the test data\n",
    "    \"\"\"\n",
    "    # Initialise predictions array\n",
    "    preds = np.zeros((X_test.shape[0], len(models)))\n",
    "    \n",
    "    # For each model weight pairing\n",
    "    for i, (learner, weight) in enumerate(models):\n",
    "        \n",
    "        # Makes weighted prediction\n",
    "        current_pred = learner.predict(X_test)\n",
    "        preds[:, i] = weight * current_pred\n",
    "    \n",
    "    # Extracts class\n",
    "    predictions = np.sign(np.sum(preds, axis = 1)).astype(int)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "class AdaBoostClassifier():\n",
    "    # Class uses previously defined 'train_ab' and 'test_ab'\n",
    "    def __init__(self, max_depth = 1):\n",
    "        self.max_depth = max_depth\n",
    "    def fit(self, X, y):\n",
    "        self.models = train_ab(X, y, self.max_depth)\n",
    "    def predict(self, X):\n",
    "        return test_ab(X, self.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- RANDOM FOREST CLASSIFIER + BOOTSTRAPPING FUNCTIONS ----\n",
    "\n",
    "def make_bootstrap(data_matrix, targets):\n",
    "    \"\"\" Function generates bootstrapped replicate of the input dataset by sampling instances with replacement. The function then extracts the oob instances\n",
    "    which are not included in the bootstrapped replicate.\n",
    "\n",
    "    Args:\n",
    "        data_matrix (array): Data matrix where each row represents an instance and each column represents a feature\n",
    "        targets (array): Target vector containing labels corresponding to each instance in data matrix\n",
    "    \n",
    "    Returns:\n",
    "        bootstrap_data_matrix (array): Bootstrapped replicate of input data matrix\n",
    "        bootstrap_targets (array): Bootstrapped replicate of input target vector\n",
    "        bootstrap_sample_ids (array): Vector containing the instance indices of boostrapped replicate of data matrix\n",
    "        oob_data_matrix (array): Data matrix containing the out-of-bag instances\n",
    "        oob_targets (array): Target Vector containing the labels of out-of-bag instances\n",
    "        oob_samples_ids (array): Vector containing the instance indices of the out-of-bag instances\n",
    "    \"\"\"\n",
    "    n_instances = len(data_matrix)\n",
    "    \n",
    "    # Generating random indices for bootstrapping\n",
    "    bootstrap_sample_ids = np.random.choice(n_instances, n_instances, replace = True)\n",
    "\n",
    "    # Creates bootstrapped replicate of data_matrix and targets using bootstrapping indices\n",
    "    bootstrap_data_matrix = data_matrix[bootstrap_sample_ids]\n",
    "    bootstrap_targets = targets[bootstrap_sample_ids]\n",
    "    \n",
    "    # Finding OOB sample ids as the ids which dont occur in bootstrap sample ids\n",
    "    oob_samples_ids = np.setdiff1d(np.arange(n_instances), bootstrap_sample_ids)\n",
    "    \n",
    "    # Selecting OOB data\n",
    "    oob_data_matrix = data_matrix[oob_samples_ids]\n",
    "    oob_targets = targets[oob_samples_ids]\n",
    "    \n",
    "    return bootstrap_data_matrix, bootstrap_targets, bootstrap_sample_ids, oob_data_matrix, oob_targets, oob_samples_ids\n",
    "\n",
    "\n",
    "def train_rfc(X_train, y_train, param):\n",
    "    \"\"\" Function trains a random forest ensemble using 100 decision tree classifiers where each tree is trained on a bootstrapped sample of the training data\n",
    "\n",
    "    Args:\n",
    "        X_train (array): Training feature matrix\n",
    "        y_train (array): Training labels\n",
    "        param (int): Maximum depth of decision trees used in ensemble\n",
    "    \n",
    "    Returns: \n",
    "        random_forest (list): List of trained decision tree classifiers (the Random Forest Ensemble)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise list to store trained decision trees\n",
    "    random_forest = []\n",
    "    \n",
    "    # We use 100 weak learners but this choice is just preference\n",
    "    for _ in range(100):\n",
    "        \n",
    "        # Create bootstrapped sample of the training data (dont need the other outputs)\n",
    "        sample, sample_targets, _, _, _, _ = make_bootstrap(X_train, y_train)\n",
    "        \n",
    "        # Trains decision tree and appends to random forest list\n",
    "        tree = DecisionTreeClassifier(max_depth = param)\n",
    "        tree.fit(sample, sample_targets) \n",
    "        random_forest.append(tree)\n",
    "        \n",
    "    return random_forest\n",
    "\n",
    "def test_rfc(X_test, models):\n",
    "    \"\"\" Function makes predictions on the test data using the trained Random Forest ensemble\n",
    "\n",
    "    Args:\n",
    "        X_test (array): Test feature matrix\n",
    "        models (list): List of trained decision tree classifiers obtained from 'train_rfc'\n",
    "    \n",
    "    Returns: \n",
    "        predictions (array): Predicted labels for the test data\n",
    "    \"\"\"\n",
    "    # Initialising array to store model predictions\n",
    "    preds = np.zeros((len(models), X_test.shape[0]))\n",
    "    \n",
    "    # For each model\n",
    "    for i, model in enumerate(models):\n",
    "        # Predicts using model and stores prediction\n",
    "        preds[i][:] = model.predict(X_test)\n",
    "        \n",
    "    # Calculates mean prediction along rows\n",
    "    mean_preds = np.mean(preds, axis = 0)\n",
    "    # Converts to binary predictions and converts 0 to -1 to work with other functions\n",
    "    predictions = (mean_preds > 0).astype(int)\n",
    "    predictions = np.where(predictions == 0, -1, predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "class RandomForestClassifier():\n",
    "    # Class using previously defined 'train_rfc', 'test_rfc'\n",
    "    def __init__(self, max_depth = 1):\n",
    "        self.max_depth = max_depth\n",
    "    def fit(self, X, y):\n",
    "        self.models = train_rfc(X, y, self.max_depth)\n",
    "    def predict(self, X):\n",
    "        return test_rfc(X, self.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- OVO CLASSIFIER ----\n",
    "\n",
    "def train_OvO(X_train, y_train, estimator):\n",
    "    \"\"\" Function trains binary classifiers for all pairs of classes using specified base estimator\n",
    "    \n",
    "    Args:\n",
    "        X_train (array): Feature matrix of shape (n_samples, n_features) \n",
    "        y_train (array): Labels of shape (n_samples)\n",
    "        estimator (): The base estimator used for training the OvO classifier\n",
    "  \n",
    "    Returns:\n",
    "        estimators (dict): Dictionary containing the trained binary classifiers for each pair of classes\n",
    "\n",
    "    \"\"\"\n",
    "    # Randomly necessary for checkpoint at end of notebook\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # Initialising dictionary and finding unique classes\n",
    "    estimators = {}\n",
    "    classes = sorted(set(y_train))\n",
    "    \n",
    "    # For all unique pairs (ignoring where i=j)\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(i + 1, len(classes)):\n",
    "            ci, cj = classes[i], classes[j]\n",
    "            \n",
    "            # Finding data and labels pertaining to current classes\n",
    "            mask = np.logical_or(y_train == ci, y_train == cj)\n",
    "            X = X_train.copy()\n",
    "            y = y_train.copy()\n",
    "            X = X[mask]\n",
    "            y = y[mask]\n",
    "  \n",
    "            # Changing classes to {-1,1}\n",
    "            yt = y.copy()\n",
    "            yt[y == ci] = 1\n",
    "            yt[y == cj] = -1\n",
    "            \n",
    "            # Training estimator for binary classification of current class pair\n",
    "            est = copy.deepcopy(estimator)\n",
    "            est.fit(X, yt)\n",
    "\n",
    "            # Storing the trained estimator to dictionary\n",
    "            estimators[(i, j)] = est\n",
    "\n",
    "    return estimators\n",
    "\n",
    "def test_OvO(X_test, estimators):\n",
    "    \"\"\" Function predicts using trained binary estimators for all pairs of classes and combines results using majority voting\n",
    "\n",
    "    Args:\n",
    "        X_test (array): Feature matrix of shape (n_samples, n_features)\n",
    "        estimators (dict): Dictionary containing trained binary classifiers for each pair of classes\n",
    "    \n",
    "    Returns:\n",
    "        preds (array): Predicted labels for test data\n",
    "    \"\"\"\n",
    "    # Randomly necessary for checkpoint at end of notebook\n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    # Initialising all predictions array\n",
    "    unique_classes = set(class_index for class_pair in estimators.keys() for class_index in class_pair)\n",
    "    scores = np.zeros((len(X_test), len(unique_classes)))\n",
    "    \n",
    "    # For each binary estimator\n",
    "    for i, j in estimators: \n",
    "        est = estimators[(i, j)]\n",
    "        \n",
    "        # Uses binary classifier to predict between pair of classes\n",
    "        pred = est.predict(X_test)\n",
    "        \n",
    "        # Need to ensure keep i,j - 1,-1 consistent\n",
    "        scores[:,i][pred == 1] += 1\n",
    "        scores[:,j][pred == -1] += 1\n",
    "        \n",
    "    preds = np.argmax(scores, axis = 1)\n",
    "    return preds\n",
    "   \n",
    "class OVOClassifier():\n",
    "    # Provides wrapper for training and testing the OvO classifier using the specified base estimator\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "    def fit(self, X, y):\n",
    "        self.estimators = train_OvO(X, y, self.estimator)\n",
    "    def predict(self, X):\n",
    "        return test_OvO(X, self.estimators)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
